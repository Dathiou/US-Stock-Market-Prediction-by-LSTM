{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def enviorment_check(id):\n",
    "    dir = '/Users/christsai/Google 雲端硬碟/US_Stock_Prediction/history/'\n",
    "    if os.path.exists(dir) == False:\n",
    "        raise IndexError('Can not find the default folder')\n",
    "    save_folder = str(dir) + str(id) \n",
    "    os.makedirs(save_folder)\n",
    "    return (save_folder + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define get data function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import quandl\n",
    "\n",
    "def GetStockData_PriceVolume(paras):\n",
    "    '''\n",
    "    All data is from quandl wiki dataset\n",
    "    Feature set: [Open  High    Low  Close    Volume  Ex-Dividend  Split Ratio Adj. Open  Adj. High  Adj. Low  \n",
    "    Adj. Close  Adj. Volume]\n",
    "    '''\n",
    "    \n",
    "    # Prepare data frame\n",
    "    stkname = \"WIKI/\" + str(paras.ticker)\n",
    "    df = quandl.get(stkname, authtoken = '2c24stWyXfdzLVFWxGe4', start_date = paras.start_date, end_date = paras.end_date)\n",
    "    df = df[['Adj. Open',  'Adj. High',  'Adj. Low',  'Adj. Close', 'Adj. Volume']]\n",
    "    df = df.rename(columns = {\"Adj. Open\": \"open\", \"Adj. High\": \"high\", \"Adj. Low\": \"low\",\n",
    "                            \"Adj. Close\": \"close\", \"Adj. Volume\": \"volume\"})\n",
    "    df_all = df.copy()\n",
    "    df['MA'] = df['close'].rolling(window = paras.pred_len, center = False).mean()\n",
    "    #df['MA'] = pd.DataFrame({'MA': df['close'].rolling(window = paras['pred_len'],center = False).mean() })\n",
    "    \n",
    "    # Data frame output\n",
    "    if paras.out_class_type == 'regression':\n",
    "        if paras.out_type == 'MA':\n",
    "            df['label'] = df['MA'].shift(-1 * paras.pred_len)\n",
    "        else:\n",
    "            df['label'] = df['close'].shift(-1 * paras.pred_len)\n",
    "    else: # classification - FIXME\n",
    "        df['label'] = df['close'].shift(-1 * paras.pred_len)\n",
    "    \n",
    "    # Generate input features for time series data\n",
    "    featureset = list(['label'])\n",
    "    featuresDict = {'c':'close', 'h':'high', 'l':'low', 'o':'open', 'v':'volume'}\n",
    "    for i in range(paras.window_len, -1, -1):\n",
    "        for j in list(paras.features):\n",
    "            df[j+'_-'+str(i)+'_d'] = df[featuresDict[j]].shift(1*i)\n",
    "            featureset.append(j+'_-'+str(i)+'_d')\n",
    "    \n",
    "    df = df[featureset]\n",
    "    df_lately = df[-paras.pred_len:]\n",
    "    df.dropna(inplace = True)\n",
    "    df_known_lately = df[len(df) - paras.known_lately_len: len(df)]\n",
    "    df = df[0:len(df)-paras.known_lately_len]\n",
    "\n",
    "    if paras.debug_load_data == True:\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        print (' %%%%%%%%%%%%%%% GetStockData PriceVolume Parameters Set %%%%%%%%%%%%%%% ')\n",
    "        print (paras)\n",
    "        print (' %%%%%%%%%%%%%%% GetStockData PriceVolume Feature Set %%%%%%%%%%%%%%% ')\n",
    "        print (featureset)\n",
    "        print (' %%%%%%%%%%%%%%% GetStockData PriceVolume df Statistic %%%%%%%%%%%%%%% ')\n",
    "        print (df.describe())\n",
    "        print (' %%%%%%%%%%%%%%% GetStockData PriceVolume df_known_lately Statistic %%%%%%%%%%%%%%% ')\n",
    "        print (df_known_lately.describe())\n",
    "        print (' %%%%%%%%%%%%%%% GetStockData PriceVolume df_lately Statistic %%%%%%%%%%%%%%% ')\n",
    "        print (df_lately.describe())\n",
    "    \n",
    "    return df, df_known_lately, df_lately, df_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalization/ preprocessing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def preprocessing_data_by_row(data, paras):\n",
    "    '''\n",
    "    data: N*M np.array\n",
    "    N: sample\n",
    "    M: features\n",
    "    data_T: M*N\n",
    "    data_T_scale: scaler for column by column, M*N\n",
    "    data_T_scale_T: N*M\n",
    "    '''\n",
    "    if data.size == 0:\n",
    "        return data, None\n",
    "    \n",
    "    data_T = data.transpose()\n",
    "    if paras.preproc_scaler == 'standard_scaler':\n",
    "        scaler = preprocessing.StandardScaler().fit(data_T)\n",
    "    else: #FIXME\n",
    "        scaler = preprocessing.StandardScaler().fit(data_T)\n",
    "    data_T_scale = scaler.transform(data_T)\n",
    "    data_T_scale_T = data_T_scale.transpose()\n",
    "    return data_T_scale_T, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_into_price_volume(df, paras):\n",
    "    '''\n",
    "    df.columns = [..., o_-10_d,h_-10_d,l_-10_d,c_-10_d,v_-10_d,...]\n",
    "    return [...,o_-10_d,h_-10_d,l_-10_d,c_-10_d,...], [...,v_-10_d,...]\n",
    "    '''\n",
    "    volume_cols = [col for col in df.columns if 'v_' in col]\n",
    "    return np.array(df.drop(volume_cols, 1)), np.array(df[volume_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_data(df, paras, featureDropForTraining, withLabel = True):\n",
    "    '''\n",
    "    df: pd.DataFrame\n",
    "    X: np.array\n",
    "    y: np.array\n",
    "    convert df into X,y\n",
    "    '''\n",
    "    y = np.array(df['label'])\n",
    "    X_price, X_volume = divide_into_price_volume(df.drop(featureDropForTraining, 1), paras)\n",
    "    \n",
    "    X_price, scaler_price = preprocessing_data_by_row(X_price, paras)\n",
    "    X_volume, scaler_volume = preprocessing_data_by_row(X_volume, paras)\n",
    "    \n",
    "    # combine price and volume - rearrange\n",
    "    # [...,o_-10_d,h_-10_d,l_-10_d,c_-10_d,...], [...,v_-10_d,...] -> [..., o_-10_d,h_-10_d,l_-10_d,c_-10_d,v_-10_d,...]\n",
    "    X_combined = X_price\n",
    "    if len(X_volume[0]) != 0:\n",
    "        for i in range(len(X_volume[0])-1, -1, -1):\n",
    "            X_combined = np.insert(X_combined, (i+1)*(paras.n_features-1), X_volume[:,i], axis = 1)\n",
    "\n",
    "    if withLabel == True:\n",
    "        y_normalized = scaler_price.transform(y.reshape(1,-1))    \n",
    "        y_normalized_T = y_normalized.reshape(-1,1)\n",
    "    else:\n",
    "        y_normalized_T = np.repeat(float('nan'), len(y))\n",
    "    scaler_combined = {'price': scaler_price, 'volume': scaler_volume}\n",
    "    \n",
    "    if paras.debug_preproc_data == True:\n",
    "        print ('df[-1:]:',df[-1:])\n",
    "        \n",
    "        if scaler_price != None:\n",
    "            print ('scaler_price len:', np.shape(scaler_price.mean_))\n",
    "            print ('scaler_price mean:',scaler_price.mean_)\n",
    "            print ('scaler_price var:',scaler_price.var_)\n",
    "            print ('normalized price:',X_price[-1:])\n",
    "            print ('inversed price:',scaler_price.inverse_transform(X_price.transpose())[:,-1])\n",
    "\n",
    "        if scaler_volume != None:\n",
    "            print ('scaler_volume len:', np.shape(scaler_volume.mean_))\n",
    "            print ('scaler_volume mean:',scaler_volume.mean_)\n",
    "            print ('scaler_volume var:',scaler_volume.var_)\n",
    "            print ('normalized volume:',X_volume[-1:])\n",
    "            print ('inversed volume:',scaler_volume.inverse_transform(X_volume.transpose())[:,-1])\n",
    "\n",
    "        print ('X_price shape',X_price.shape)\n",
    "        print ('X_volume shape',X_volume.shape)\n",
    "        print ('X_combined shape',X_combined.shape)\n",
    "        print ('X_combined',X_combined[-1:])\n",
    "        \n",
    "        print ('y[-1:]:', y[-1:])\n",
    "        print ('y[-1] normalized:', y_normalized[-1:])\n",
    "        print ('y[-1] inversed:', scaler_price.inverse_transform(y_normalized)[:,-1])\n",
    "    \n",
    "    return X_combined, y_normalized_T, scaler_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape_input(X, y, paras):\n",
    "    n_sample = X.shape[0]\n",
    "    n_channel = paras.n_features\n",
    "    n_features_per_channel = X.shape[1]/n_channel\n",
    "    X_reshaped = np.reshape(X, (n_sample, n_features_per_channel, n_channel))\n",
    "    y_reshaped = np.reshape(y, (n_sample,-1))\n",
    "    return X_reshaped, y_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "def build_LSTM_model(paras):\n",
    "    model_lstm = Sequential()\n",
    "    first = True\n",
    "    for idx in range(len(paras.model['hidden_layers'])):\n",
    "        if idx == (len(paras.model['hidden_layers']) - 1):\n",
    "            model_lstm.add(LSTM(paras.model['hidden_layers'][idx], return_sequences = False))\n",
    "            model_lstm.add(Activation(paras.model['activation'][idx]))\n",
    "            model_lstm.add(Dropout(paras.model['dropout'][idx]))\n",
    "        elif first == True:\n",
    "            model_lstm.add(LSTM(input_dim = paras.n_features,\n",
    "                                output_dim = paras.model['hidden_layers'][idx],\n",
    "                                return_sequences = True))\n",
    "            model_lstm.add(Activation(paras.model['activation'][idx]))\n",
    "            model_lstm.add(Dropout(paras.model['dropout'][idx]))\n",
    "            first = False\n",
    "        else:\n",
    "            model_lstm.add(LSTM(paras.model['hidden_layers'][idx], return_sequences = True))\n",
    "            model_lstm.add(Activation(paras.model['activation'][idx]))\n",
    "            model_lstm.add(Dropout(paras.model['dropout'][idx]))\n",
    "   \n",
    "    # output layer\n",
    "    model_lstm.add(Dense(output_dim = paras.model['out_layer']))\n",
    "    model_lstm.add(Activation(paras.model['out_activation']))\n",
    "    model_lstm.compile(loss = paras.model['loss'], optimizer = paras.model['optimizer'])\n",
    "    print ('build LSTM model...')\n",
    "    return model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LSTM_model_predict(model, X, y, scaler = None):\n",
    "    predictions = model.predict(X)\n",
    "    mse_scaled = np.mean((y - predictions) ** 2)\n",
    "    print ('scaled data mse: ', mse_scaled)\n",
    "    \n",
    "    if scaler != None:\n",
    "        arr = np.array(scaler.inverse_transform(y.reshape(y.shape[0],)))\n",
    "        arr2 = np.array(scaler.inverse_transform(predictions.reshape(predictions.shape[0],)))\n",
    "        return mse_scaled, arr, arr2\n",
    "    return mse_scaled, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_training_model(model, paras, name):\n",
    "    # https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
    "    model.save(paras.save_folder + name + '.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "def append_date_serires(df, paras):\n",
    "    append_date = []\n",
    "    append_last_date = df.index[-1]\n",
    "    i = paras.pred_len\n",
    "    while i >= 1:\n",
    "        append_last_date = append_last_date + timedelta(days=1)\n",
    "        if append_last_date.isoweekday() > 0 and append_last_date.isoweekday() < 6:\n",
    "            append_date.append(append_last_date)\n",
    "            i -= 1\n",
    "    append_df = pd.DataFrame(index = list(append_date))\n",
    "    df = pd.concat((df, append_df), axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_data_frame_mse(df, paras, mses):\n",
    "    df = append_date_serires(df, paras)\n",
    "    \n",
    "    if 1:\n",
    "        df['actual'] = df['actual']\n",
    "        df['pred'] = df['pred']\n",
    "        df = df.rename(columns = {\"actual\": \"a_+\"+str(paras.pred_len)+'_d',\n",
    "                                  \"pred\": \"p_+\"+str(paras.pred_len)+'_d'})\n",
    "        \n",
    "        df['a_+'+str(paras.pred_len)+'_d_diff'] = df[\"a_+\"+str(paras.pred_len)+'_d'] - df['close']\n",
    "        df['p_+'+str(paras.pred_len)+'_d_diff'] = df[\"p_+\"+str(paras.pred_len)+'_d'] - df['close']\n",
    "        new_list = [\"a_+\"+str(paras.pred_len)+'_d', \"p_+\"+str(paras.pred_len)+'_d', \n",
    "                    'a_+'+str(paras.pred_len)+'_d_diff', 'p_+'+str(paras.pred_len)+'_d_diff']\n",
    "    else :\n",
    "        # shift +\n",
    "        df['actual'] = df['actual'].shift(paras.pred_len)\n",
    "        df['pred'] = df['pred'].shift(paras.pred_len)\n",
    "        df = df.rename(columns = {\"actual\": \"a_-\"+str(paras.pred_len)+'_d',\n",
    "                                  \"pred\": \"p_-\"+str(paras.pred_len)+'_d'})\n",
    "        new_list = [\"a_-\"+str(paras.pred_len)+'_d', \"p_-\"+str(paras.pred_len)+'_d']\n",
    "    \n",
    "    default_list = ['open', 'high', 'low', 'close', 'volume']\n",
    "    original_other_list = set(df.columns) - set(default_list) - set(new_list)\n",
    "    original_other_list = list(original_other_list)\n",
    "    df = df[default_list + original_other_list + new_list]\n",
    "    model_acc = mses[1] / mses[0]\n",
    "    df.to_csv(paras.save_folder + paras.ticker + ('_%.2f' % model_acc)+ \"_data_frame.csv\")\n",
    "    with open(paras.save_folder + 'parameters.txt', \"w\") as text_file:\n",
    "        text_file.write(paras.__str__())\n",
    "        text_file.write(str(mses[0]) + '\\n')\n",
    "        text_file.write(str(mses[1]) + '\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "# http://matplotlib.org/examples/pylab_examples/subplots_demo.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "    \n",
    "def plot_training_curve(history, paras, save = True):\n",
    "    # LSTM model plot\n",
    "    # from keras.utils.visualize_util import plot\n",
    "    # from IPython.display import SVG\n",
    "    # from keras.utils.visualize_util import model_to_dot\n",
    "    # plot(model_lstm, to_file='model.png')\n",
    "    # SVG(model_to_dot(model_lstm).create(prog='dot', format='svg'))\n",
    "    \n",
    "    %matplotlib inline\n",
    "    # Control the default size of figures in this Jupyter notebook\n",
    "    %pylab inline\n",
    "    pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots\n",
    "\n",
    "    # LSTM training \n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot(history.history['loss'])\n",
    "    ax.plot(history.history['val_loss'])\n",
    "    ax.set_title('loss function')\n",
    "    ax.set_ylabel('mse')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    plt.show()\n",
    "    if save == True:\n",
    "        savefig(paras.save_folder + 'training_curve.png')\n",
    "        w = csv.writer(open(paras.save_folder + \"training_curve_model.txt\", \"w\"))\n",
    "        for key, val in history.history.items():\n",
    "            w.writerow([key, val])\n",
    "        for key, val in history.params.items():\n",
    "            w.writerow([key, val])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# thanks for : https://ntguardian.wordpress.com/2016/09/19/introduction-stock-market-data-python-1/\n",
    "from matplotlib.dates import DateFormatter, WeekdayLocator, DayLocator, MONDAY\n",
    "from matplotlib.finance import candlestick_ohlc\n",
    "\n",
    "def plot_result(df_total, paras, save = True):\n",
    "#     df_total = pd.concat((df[-paras.window_len:], df_known_lately, df_lately), axis = 0)\n",
    "#     df_total = append_date_serires(df_total, paras)\n",
    "    \n",
    "#     df_total['pred'] = df_total['pred'].shift(paras.pred_len)\n",
    "#     df_total['val'] = df_total['val'].shift(paras.pred_len)\n",
    "    \n",
    "    mondays = WeekdayLocator(MONDAY)        # major ticks on the mondays\n",
    "    alldays = DayLocator()              # minor ticks on the days\n",
    "    dayFormatter = DateFormatter('%d')      # e.g., 12\n",
    "    \n",
    "    # plot candleStick/ line for all data\n",
    "    # Set plot parameters, including the axis object ax used for plotting\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    if df_total.index[-1] - df_total.index[0] < pd.Timedelta('730 days'):\n",
    "        weekFormatter = DateFormatter('%b %d')  # e.g., Jan 12\n",
    "        ax.xaxis.set_major_locator(mondays)\n",
    "        ax.xaxis.set_minor_locator(alldays)\n",
    "    else:\n",
    "        weekFormatter = DateFormatter('%b %d, %Y')\n",
    "    ax.xaxis.set_major_formatter(weekFormatter)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    if set(['h','l','c','o']) <= set(list(paras.features)):\n",
    "        # plot candle stick\n",
    "        # Create the candelstick chart\n",
    "        candlestick_ohlc(ax, list(zip(list(date2num(df_total.index.tolist())), \n",
    "                                      df_total[\"o_-0_d\"].tolist(), df_total[\"h_-0_d\"].tolist(),\n",
    "                                      df_total[\"l_-0_d\"].tolist(), df_total[\"c_-0_d\"].tolist())),\n",
    "                         colorup = \"green\", colordown = \"red\", width = 0.4)\n",
    "    else:\n",
    "        # plot line\n",
    "        df_total.loc[:,['c_-0_d']].plot(ax = ax, lw = 1.3, grid = True, legend = ['close'])\n",
    "\n",
    "    # plot prediction and true value\n",
    "    df_total.loc[:,['pred', 'val']].plot(ax = ax, lw = 1.3, grid = True, legend = ['pred', 'val'])\n",
    "\n",
    "    ax.xaxis_date()\n",
    "    ax.autoscale_view()\n",
    "    plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    plt.ion()\n",
    "    \n",
    "    \n",
    "    # add annotations\n",
    "    ax.annotate('known lately start', (date2num(df_known_lately.index.tolist()[0]), \n",
    "                                       df_known_lately[\"c_-0_d\"].tolist()[0]), \n",
    "                xytext=(15, 15), textcoords='offset points', \n",
    "                arrowprops=dict(arrowstyle='-|>'))\n",
    "    \n",
    "    ax.annotate('lately start', (date2num(df_lately.index.tolist()[0]), \n",
    "                                       df_lately[\"c_-0_d\"].tolist()[0]), \n",
    "                xytext=(15, 15), textcoords='offset points', \n",
    "                arrowprops=dict(arrowstyle='-|>'))\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    if save == True:\n",
    "        savefig(paras.save_folder + 'pred_result.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from keras.callbacks import History \n",
    "\n",
    "def run_LSTM(paras):\n",
    "    ################################################################################\n",
    "    paras.save_folder = enviorment_check(paras.identify)\n",
    "    print ('save directory: ', paras.save_folder)\n",
    "    ################################################################################\n",
    "\n",
    "    featureDropForTraining = ['label']\n",
    "\n",
    "    # get data\n",
    "    df, df_known_lately, df_lately, df_all = GetStockData_PriceVolume(paras)\n",
    "\n",
    "    # preprocessing\n",
    "    X, y, scaler = preprocessing_data(df, paras, featureDropForTraining, withLabel = True)\n",
    "    X_known_lately, y_known_lately, scaler_known_lately = preprocessing_data(df_known_lately, paras, featureDropForTraining, withLabel = True)\n",
    "    X_lately, y_lately, scaler_lately = preprocessing_data(df_lately, paras, featureDropForTraining, withLabel = False)\n",
    "\n",
    "    # cross validation\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)\n",
    "    print ('train shape',X_train.shape)\n",
    "    print ('test shape',X_test.shape)\n",
    "\n",
    "    # reshape input data to LSTM model\n",
    "    X_train, y_train = reshape_input(X_train, y_train, paras)\n",
    "    X_test, y_test = reshape_input(X_test, y_test, paras)\n",
    "    X_known_lately, y_known_lately = reshape_input(X_known_lately, y_known_lately, paras)\n",
    "    X_lately, y_lately = reshape_input(X_lately, y_lately, paras)\n",
    "\n",
    "    # build LSTM model\n",
    "    history = History()\n",
    "    model_lstm = build_LSTM_model(paras)\n",
    "    model_lstm.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size = paras.batch_size,\n",
    "        nb_epoch = paras.epoch,\n",
    "        validation_split = paras.validation_split,\n",
    "        #validation_data = (X_known_lately, y_known_lately),\n",
    "        callbacks=[history],\n",
    "        verbose = 1\n",
    "    )\n",
    "    # save model\n",
    "    save_training_model(model_lstm, paras, 'lstm_model')\n",
    "\n",
    "    # validation test + known lately data \n",
    "    print (' ############## validation on test data ############## ')\n",
    "    mse_test, tmp, tmp2 = LSTM_model_predict(model_lstm, X_test, y_test)\n",
    "\n",
    "    print (' ############## validation on known lately data ############## ')\n",
    "    mse_known_lately, df_all.loc[df_known_lately.index, 'actual'], df_all.loc[df_known_lately.index, 'pred'] = LSTM_model_predict(model_lstm, X_known_lately, y_known_lately, \n",
    "                                                      scaler = scaler_known_lately['price'])\n",
    "    # predict lately data\n",
    "    print (' ############## validation on lately data ############## ')\n",
    "    mse_lately, df_all.loc[df_lately.index, 'actual'], df_all.loc[df_lately.index, 'pred'] = LSTM_model_predict(model_lstm, X_lately, y_lately, \n",
    "                                       scaler = scaler_lately['price'])\n",
    "\n",
    "    # rewrite data frame and save / update\n",
    "    df_all = save_data_frame_mse(df_all, paras, mses = [mse_test, mse_known_lately])\n",
    "\n",
    "    # plot training loss/ validation loss\n",
    "    plot_training_curve(history, paras, save = True)\n",
    "\n",
    "    # # plot result\n",
    "    # show_len = paras.window_len + paras.known_lately_len + paras.pred_len*2\n",
    "    # plot_result(df_all[-show_len:], paras, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save directory:  /Users/christsai/Google 雲端硬碟/US_Stock_Prediction/history/LSTM_NDAQ_2017-03-03 15:30:10/\n",
      "train shape (1315, 605)\n",
      "test shape (329, 605)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christsai/anaconda3/lib/python3.5/site-packages/numpy/core/fromnumeric.py:225: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return reshape(newshape, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build LSTM model...\n",
      "Train on 1183 samples, validate on 132 samples\n",
      "Epoch 1/50\n",
      "1183/1183 [==============================] - 23s - loss: 1.6694 - val_loss: 1.0310\n",
      "Epoch 2/50\n",
      "1183/1183 [==============================] - 19s - loss: 1.0379 - val_loss: 0.8203\n",
      "Epoch 3/50\n",
      "1183/1183 [==============================] - 19s - loss: 0.8494 - val_loss: 1.3285\n",
      "Epoch 4/50\n",
      "1183/1183 [==============================] - 20s - loss: 0.8681 - val_loss: 0.7523\n",
      "Epoch 5/50\n",
      " 512/1183 [===========>..................] - ETA: 11s - loss: 0.6707"
     ]
    }
   ],
   "source": [
    "import US_SP_Global_Parameters\n",
    "if __name__ == \"__main__\":\n",
    "    # #################### parameters setting ####################\n",
    "    paras = US_SP_Global_Parameters.SP_RNN_LSTM_Paras('LSTM', ticker = 'NDAQ')\n",
    "    paras.features = 'ohlcv'\n",
    "    paras.window_len = 120\n",
    "    paras.pred_len = 20\n",
    "    paras.known_lately_len = 20\n",
    "    paras.debug_load_data = False\n",
    "    paras.debug_preproc_data = False\n",
    "\n",
    "    # LSTM parameters\n",
    "    paras.batch_size = 128\n",
    "    paras.epoch = 50\n",
    "    paras.validation_split = 0.1\n",
    "    paras.model['hidden_layers'] = [120, 60, 30]\n",
    "    paras.model['dropout'] = [0.5, 0.5, 0.3]\n",
    "    paras.model['activation'] = ['relu', 'relu', 'relu']\n",
    "    paras.model['out_layer'] = 1\n",
    "    paras.model['out_activation'] = 'linear'\n",
    "    paras.model['loss'] = 'mse'\n",
    "    paras.model['optimizer'] = 'rmsprop'\n",
    "\n",
    "    run_LSTM(paras)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
